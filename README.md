<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Python-3.8+-3776ab?logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  <img src="https://img.shields.io/badge/CUDA-12.0+-76b900?logo=nvidia&logoColor=white" alt="CUDA">
</p>

<h1 align="center">ğŸŒŠ Adaptive Training Framework (ATF)</h1>

<p align="center">
  <strong>Resonance-based neural network training optimization</strong><br>
  <em>Achieve faster convergence, prevent training collapse, and save 40-70% compute</em>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-results">Results</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-gui-dashboard">GUI Dashboard</a> â€¢
  <a href="#-modules">Modules</a> â€¢
  <a href="#-cli-reference">CLI Reference</a>
</p>

---

## ğŸ¯ What is ATF?

ATF is a **modular training optimization framework** that applies resonance-based principles to neural network training. It monitors convergence patterns, adapts learning rates dynamically, and prevents catastrophic training failures.

### Key Discovery: Universal Frequency Ï‰ â‰ˆ 6.0

Through extensive experimentation across vision and NLP tasks, we discovered that an angular frequency of **Ï‰ â‰ˆ 6.0** consistently produces optimal training dynamics across different domains.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ATF Training Flow                            â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ Training â”‚â”€â”€â”€â”€â–¶â”‚Convergticâ”‚â”€â”€â”€â”€â–¶â”‚   Meta   â”‚              â”‚
â”‚   â”‚   Loop   â”‚     â”‚ Analysis â”‚     â”‚Controllerâ”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚        â”‚                â”‚                â”‚                      â”‚
â”‚        â–¼                â–¼                â–¼                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ Periodic â”‚     â”‚Convergticâ”‚     â”‚  Early   â”‚              â”‚
â”‚   â”‚    LR    â”‚     â”‚  Damper  â”‚     â”‚   Stop   â”‚              â”‚
â”‚   â”‚  Ï‰=6.0   â”‚     â”‚          â”‚     â”‚          â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                 â”‚
â”‚   Result: Faster convergence + Stability + Compute savings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Results

### Vision Tasks

| Dataset | Baseline | ATF | Î” Accuracy | Î” Time | Winner |
|---------|----------|-----|------------|--------|--------|
| **CIFAR-100** | 68.58% | **69.33%** | **+0.75%** | **-59%** | ğŸ† ATF |
| **CIFAR-10** | 90.54% | 90.45% | -0.09% | **-36%** | â±ï¸ ATF |
| MNIST | 99.44% | 99.33% | -0.11% | â‰ˆ | Tie |
| Fashion-MNIST | 92.59% | 92.80% | +0.21% | -23% | ATF |

### NLP Tasks (BERT Fine-tuning)

| Task | Dataset Size | Baseline | ATF | Î” Accuracy | Winner |
|------|-------------|----------|-----|------------|--------|
| **MRPC** | 3.7k (small) | 84.80% | **87.25%** | **+2.45%** | ğŸ† ATF |
| SST-2 | 67k (large) | **93.23%** | 92.78% | -0.45% | Baseline |

### ğŸ“ˆ Benchmark Charts

<p align="center">
  <img src="assets/accuracy_comparison.png" alt="Model Accuracy: ATF vs Baseline" width="700">
</p>

<p align="center">
  <em>ATF achieves higher accuracy on complex tasks (CIFAR-100) and small datasets (MRPC)</em>
</p>

<p align="center">
  <img src="assets/training_time_comparison.png" alt="Training Time: ATF vs Baseline" width="700">
</p>

<p align="center">
  <em>ATF reduces training time by 36-59% through intelligent early stopping</em>
</p>

### Key Findings

```
ATF Performance Summary:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Best for: Complex tasks (CIFAR-100), small datasets (MRPC)
âœ… Compute savings: 36-59% faster training
âœ… Prevents: Training collapse, catastrophic forgetting
âš ï¸ Not optimal for: Pre-trained models on large datasets
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Training Collapse Prevention

On CIFAR-10, baseline training without ATF can collapse to near-random performance:

```
Without ATF:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40.65% (collapsed!)
With ATF:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 80.03% (stable)
                                   â†‘ ATF prevented collapse
```

---

## ğŸš€ Quick Start

### Basic Usage

```python
from atf import ATFOrchestrator, TrainingConfig
import torch

# Configure ATF
config = TrainingConfig(
    lr_omega=6.0,           # Universal frequency
    lr_amplitude=0.08,      # LR oscillation amplitude
    ca_patience=5,          # Epochs before LR reduction
    use_early_stopping=True
)

# Create orchestrator
orchestrator = ATFOrchestrator(optimizer, config)

# Training loop
for epoch in range(epochs):
    orchestrator.on_epoch_start(epoch)
    
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        loss = criterion(model(data), target)
        
        # ATF step - modulates loss and learning rate
        loss = orchestrator.on_train_step(model, loss, global_step)
        
        loss.backward()
        optimizer.step()
    
    # End of epoch - evaluate and check for early stop
    actions, metrics = orchestrator.on_eval_end(epoch, val_loss)
    
    if actions.get('stop'):
        print(f"Early stopping at epoch {epoch}")
        break
```

### One-liner with CLI

```bash
# Train CIFAR-10 with ATF
python -m atf.cli.run --dataset cifar10 --epochs 50 --atf

# Train BERT on MRPC
python -m atf.cli.run --dataset bert_mrpc --epochs 5 --atf --patience 3
```

---

## ğŸ“¦ Installation

### From Source

```bash
git clone https://github.com/Freeky7819/Adaptive_Training_Framework.git
cd Adaptive_Training_Framework
pip install -e .
```

### Requirements

```bash
pip install -r requirements.txt
```

**Core dependencies:**
- Python >= 3.8
- PyTorch >= 2.0
- NumPy
- tqdm

**Optional (for NLP):**
- transformers
- datasets

**Optional (for GUI):**
- fastapi
- uvicorn
- websockets

---

## ğŸ–¥ï¸ GUI Dashboard

ATF includes a real-time training dashboard with live metrics, module visualization, and quick configuration.

```bash
# Start the server
cd gui
python server.py

# Open in browser
# http://localhost:8000
```

### Features

- ğŸ“Š **Real-time metrics**: Loss, accuracy, learning rate, Î² convergence
- ğŸ”„ **Live architecture diagram**: See which modules are active
- âš¡ **Quick Config**: Paste CLI commands to configure instantly
- ğŸ“‹ **Presets**: One-click configurations for common tasks
- ğŸ”§ **AutoTune**: Automatic hyperparameter search

<details>
<summary>ğŸ“¸ Screenshot Preview</summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ATF Dashboard                                    ğŸŸ¢ Connected  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š Dataset   â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”       â”‚
â”‚ CIFAR-10     â”‚  â”‚92.4%â”‚ â”‚0.234â”‚ â”‚0.001â”‚ â”‚0.008â”‚ â”‚ 15  â”‚       â”‚
â”‚              â”‚  â”‚ Acc â”‚ â”‚Loss â”‚ â”‚ LR  â”‚ â”‚  Î²  â”‚ â”‚Best â”‚       â”‚
â”‚ âš¡ Quick     â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜       â”‚
â”‚ Config       â”‚                                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ğŸ“ˆ Loss        ğŸ“Š Accuracy    âš¡ Learning Rate â”‚
â”‚ â”‚--omega 6 â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚ â”‚--amp 0.08â”‚ â”‚  â”‚ â•²___   â”‚    â”‚    ___/â”‚     â”‚ ~~~~~  â”‚       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚              â”‚                                                  â”‚
â”‚ âš¡ Modules   â”‚  ğŸ”„ ATF Live Architecture                       â”‚
â”‚ â˜‘ Conv.Anal â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â˜‘ Periodic  â”‚  â”‚ Training â†’ GradFB â†’ ConvAnalysis â†’ Meta â”‚   â”‚
â”‚ â˜‘ Meta Ctrl â”‚  â”‚     â†“         â†“          â†“          â†“    â”‚   â”‚
â”‚ â˜ Curvature â”‚  â”‚ PeriodicLR  TempBuf   ConvDamp    Stop  â”‚   â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>

See [gui/README.md](gui/README.md) for detailed documentation.

---

## ğŸ§© Modules

ATF consists of 8 modular components that can be enabled/disabled independently:

### Core Modules

| Module | Purpose | Key Parameters |
|--------|---------|----------------|
| **Convergence Analysis (CA)** | Monitors training convergence via Î² metric | `patience`, `min_delta` |
| **Periodic LR Scheduler (PLR)** | Oscillating learning rate with decay | `omega`, `amplitude`, `decay` |
| **Meta Controller (MC)** | Adaptive LR reduction and early stopping | `lr_factor`, `patience` |
| **Convergence Damper (CD)** | Stabilizes training near convergence | `threshold`, `alpha` |

### Enhancement Modules

| Module | Purpose | Key Parameters |
|--------|---------|----------------|
| **Gradient Feedback (GF)** | Adaptive loss modulation | `alpha`, `omega`, `phi` |
| **Temporal Buffer (TB)** | Historical gradient analysis | `window_size`, `decay` |
| **Harmonic Init (HI)** | Resonance-based weight initialization | `omega`, `amplitude` |
| **Curvature Regularizer (CR)** | Second-order smoothing | `lambda`, `threshold` |

### Module Selection Guide

```
Vision Tasks (CIFAR, MNIST):
â”œâ”€â”€ Use: CA + PLR + MC + CD + GF + TB + HI
â””â”€â”€ Params: omega=6.0, amp=0.08, patience=5

BERT Fine-tuning (small datasets like MRPC):
â”œâ”€â”€ Use: CA + MC + CD + PLR
â”œâ”€â”€ Disable: HI, GF, TB  (BERT is pre-trained!)
â””â”€â”€ Params: omega=6.0, amp=0.01, patience=3

BERT Fine-tuning (large datasets like SST-2):
â””â”€â”€ Use: Baseline (ATF oscillations can hurt)
```

---

## âŒ¨ï¸ CLI Reference

### Quick Config Format

ATF GUI accepts CLI-style commands for quick configuration:

```bash
# Special flags
--baseline          # Disable all ATF modules
--atf / --full      # Enable all ATF modules

# Core parameters
--omega 6.0         # Angular frequency (Ï‰)
--amp 0.08          # LR oscillation amplitude
--decay 0.003       # Exponential decay rate
--patience 5        # Epochs without improvement before LR reduction
--max-red 2         # Max LR reductions before early stop

# Training
--epochs 50         # Number of epochs
--batch 128         # Batch size
--lr 0.001          # Base learning rate
--seed 42           # Random seed

# NLP specific
--seq-len 128       # Max sequence length (BERT)
--block-size 256    # Context window (GPT)

# Module toggles
--ca on/off         # Convergence Analysis
--gfc on/off        # Gradient Feedback
--plr on/off        # Periodic LR
--cd on/off         # Convergence Damper
--tb on/off         # Temporal Buffer
--hi on/off         # Harmonic Init
--mc on/off         # Meta Controller
--cr on/off         # Curvature Regularizer
```

### Preset Commands

| Preset | Command |
|--------|---------|
| **CIFAR-10 ATF** | `--atf --epochs 50 --omega 6.0 --amp 0.08 --patience 5` |
| **CIFAR-100 ATF** | `--atf --epochs 75 --omega 6.0 --amp 0.08 --patience 5` |
| **BERT MRPC ATF** | `--atf --epochs 5 --lr 0.00002 --amp 0.01 --patience 3 --hi off --gfc off` |
| **Conservative** | `--atf --omega 5.5 --amp 0.04 --patience 8` |
| **Aggressive** | `--atf --omega 7.0 --amp 0.12 --patience 3` |

---

## ğŸ“ Project Structure

```
Adaptive_Training_Framework/
â”œâ”€â”€ atf/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # TrainingConfig dataclass
â”‚   â”‚   â”œâ”€â”€ orchestrator.py    # Main ATFOrchestrator
â”‚   â”‚   â””â”€â”€ monitor.py         # Convergence monitoring
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ convergence_damper.py
â”‚   â”‚   â”œâ”€â”€ curvature_regularizer.py
â”‚   â”‚   â”œâ”€â”€ gradient_feedback.py
â”‚   â”‚   â”œâ”€â”€ harmonic_init.py
â”‚   â”‚   â”œâ”€â”€ meta_controller.py
â”‚   â”‚   â””â”€â”€ temporal_buffer.py
â”‚   â”œâ”€â”€ schedulers/
â”‚   â”‚   â””â”€â”€ periodic_lr.py     # Oscillating LR scheduler
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ datasets.py        # Vision datasets
â”‚   â”‚   â””â”€â”€ nlp_datasets.py    # BERT/GPT datasets
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ cnn.py             # Vision models
â”‚       â””â”€â”€ nlp.py             # NLP models
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ server.py              # WebSocket server
â”‚   â”œâ”€â”€ atf_dashboard_live.html
â”‚   â””â”€â”€ README.md              # GUI documentation
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ bert_glue.py           # BERT fine-tuning
â”‚   â””â”€â”€ nanogpt_train.py       # Language modeling
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_basic.py
â”‚   â””â”€â”€ benchmark_suite.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ”¬ Theory

ATF is based on the principle that training dynamics exhibit resonance-like behavior. By applying periodic modulation at the optimal frequency (Ï‰ â‰ˆ 6.0), we can:

1. **Escape local minima** - Oscillations help explore the loss landscape
2. **Accelerate convergence** - Resonance amplifies gradient signal
3. **Prevent overfitting** - Damping stabilizes near convergence
4. **Save compute** - Early stopping when converged

### The Î² Convergence Metric

```
Î² = EMA(|âˆ‡L|) / max(EMA(|âˆ‡L|))

Î² â†’ 0: Model converged (gradients vanishing)
Î² â†’ 1: Active learning (large gradients)
Î² oscillating: Unstable training
```

### Periodic LR Formula

```
lr(t) = lr_base Ã— (1 + Î± Ã— sin(Ï‰ Ã— t + Ï†)) Ã— exp(-k Ã— t)

Where:
- lr_base: Initial learning rate
- Î±: Oscillation amplitude (0.08 default)
- Ï‰: Angular frequency (6.0 universal optimum)
- Ï†: Phase offset
- k: Exponential decay rate
```

---

## ğŸ“– Citation

If you use ATF in your research, please cite:

```bibtex
@software{zakelj2025atf,
  author = {Å½akelj, Damjan},
  title = {Adaptive Training Framework: Resonance-based Neural Network Optimization},
  year = {2025},
  url = {https://github.com/Freeky7819/Adaptive_Training_Framework}
}
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

Developed with extensive experimentation and testing across vision and NLP domains. Special thanks to the PyTorch and Hugging Face communities.

---

<p align="center">
  <strong>Built with ğŸ’™ by Damjan Å½akelj</strong><br>
  <em>"Science = Beauty + Truth"</em>
</p>
